{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- TODO: load actual word vectors\n",
    "-- first dimension: 150 examples\n",
    "-- second dimension: 200 words OR 8 labels\n",
    "-- third dimension: word vectors of dimensionality 300\n",
    "train_inputs = torch.randn(150, 200, 300)\n",
    "train_targets = torch.randn(150, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Input format is a matrix $M \\in \\mathbb{R}^{n} \\times k$\n",
    "with:\n",
    "* $n$ number of words in the document\n",
    "* $k$ the dimension of the word vectors obtained from word2vec\n",
    "* the data is zero-padded to the length of the longest document\n",
    "(alternatively to a given maximum length). All not-taken word spaces\n",
    "are set to zero in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* Filter windows of height $h$ are applied in sizes of $3,4,5$ with TODO feature\n",
    "maps. Specifically the filters are applied as a matrix $W \\in \\mathbb{R}^{h \\times k}$\n",
    "to a window of size $h \\times k$.\n",
    "* The stride of the filters is $1$ to iterate over all possible windows of words\n",
    "(*narrow convolution*).\n",
    "* A dropout of rate $\\rho = .5$ is applied at training time to prevent overfitting.\n",
    "* An $l_2$ constraint $s = 3$ is applied in (Kim, 2014). However (Zhang and Wallace, 2015)\n",
    "found that this constraint had little effect on the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- metadata\n",
    "batch_size = 100 -- ?\n",
    "no_examples = train_inputs:size()[1]\n",
    "no_classes = train_targets:size()[2]\n",
    "sent_len = 150 -- TODO\n",
    "wordvec_len = 300\n",
    "dropout_rho = .5\n",
    "l2_constr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- we have to copy the matrix 6 times to use the parallel container\n",
    "-- also we squash the words to a given length, if not already done so.\n",
    "-- zero padding is applied as well.\n",
    "X = torch.Tensor(6, no_examples, sent_len, wordvec_len):zero()\n",
    "y = train_targets\n",
    "\n",
    "-- FIXME: this is way too complex and memory-inefficient. should use storage\n",
    "-- and stuff in the future.\n",
    "\n",
    "for i = 1, no_examples do\n",
    "    for j = 1, sent_len do\n",
    "        X[{1, i, j}] = train_inputs[i][j]\n",
    "        X[{2, i, j}] = train_inputs[i][j]\n",
    "        X[{3, i, j}] = train_inputs[i][j]\n",
    "        X[{4, i, j}] = train_inputs[i][j]\n",
    "        X[{5, i, j}] = train_inputs[i][j]\n",
    "        X[{6, i, j}] = train_inputs[i][j]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model:add(nn.Padding())\n",
    "\n",
    "-- stage 1: convolutions\n",
    "\n",
    "-- in: applies the following declared layers to the first dimension of the\n",
    "-- tensor (see above.)\n",
    "--\n",
    "-- out: concatenating the three concatenated max-pooled values to a vector\n",
    "-- fed to the fully connected softmax layer yielding the outputs\n",
    "p = nn.Parallel(1,1)\n",
    "\n",
    "-- this is a convolution unit differing in the height of the filter being\n",
    "-- applied. each filter is used double time to further improve performance.\n",
    "-- each filter yields a feature map, thus for each region size we then\n",
    "-- have two feature maps, and in total then six (if using default n-grams\n",
    "-- 3 to 5 like (Kim, 2014))\n",
    "for i = 3, 5 do\n",
    "    -- for the two convolutions we use for each region size\n",
    "    local f = nn.Parallel(1,1)\n",
    "\n",
    "    -- elements of the convolution\n",
    "    local s1 = nn.Sequential()\n",
    "    local s2 = nn.Sequential()\n",
    "\n",
    "    -- takes size of input plane (we only have one channel though)\n",
    "    -- as well as output plane (again, using only one channel)\n",
    "    -- and also the kernel width and height. in our case the width is fixed\n",
    "    -- to a row in the input matrix for the document. the height however\n",
    "    -- varies and ranges from 3..5\n",
    "    s1:add(nn.SpatialConvolution(1, 1, sent_len, i))\n",
    "    s2:add(nn.SpatialConvolution(1, 1, sent_len, i))\n",
    "\n",
    "    -- non-linearities\n",
    "    s1:add(nn.ReLU())\n",
    "    s2:add(nn.ReLU())\n",
    "\n",
    "    -- the viewed region of the matrix for max-pooling shall be the\n",
    "    -- size of the matrix, as we want all values to be considered at\n",
    "    -- once for a single maximum for each filter map.\n",
    "    s1:add(nn.SpatialMaxPooling(sent_len, i))\n",
    "    s2:add(nn.SpatialMaxPooling(sent_len, i))\n",
    "\n",
    "    f:add(s1)\n",
    "    f:add(s2)\n",
    "\n",
    "    -- concatenating the two max-pooled values to a tensor of dim TODO\n",
    "    f:add(nn.Concat(1))\n",
    "\n",
    "    p:add(f)\n",
    "end\n",
    "\n",
    "model:add(p)\n",
    "\n",
    "-- stage 2: fully connected softmax layer\n",
    "model:add(nn.Normalize(2, l2_constr))\n",
    "model:add(nn.Dropout(dropout_rho))\n",
    "-- model.add(nn.Linear(6, no_classes))\n",
    "\n",
    "-- model:add(nn.LogSoftMax()) -- for ClassNLLCriterion\n",
    "model:add(nn.SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The network is trained with Stochastic Gradient Descent (SGD)\n",
    "with randomly shuffled mini-batches and the Adadelta update rule (Zeiler, 2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_batch = function(offset)\n",
    "    end_offset = offset + batch_size\n",
    "    \n",
    "    if end_offset > X:size()[2] then\n",
    "        end_offset = offset + (X:size()[2] - offset)\n",
    "    end\n",
    "\n",
    "    return\n",
    "        X[{{}, {offset, end_offset}, {}, {}}]\n",
    "        , y[{{offset, end_offset}, {}}]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"for i = 1, no_examples, batch_size do...\"]:2: attempt to call global 'next_batch' (a nil value)\nstack traceback:\n\t[string \"for i = 1, no_examples, batch_size do...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/nexus/.ipython/profile_default/s...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"for i = 1, no_examples, batch_size do...\"]:2: attempt to call global 'next_batch' (a nil value)\nstack traceback:\n\t[string \"for i = 1, no_examples, batch_size do...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/nexus/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/nexus/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/nexus/.ipython/profile_default/s...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "for i = 1, no_examples, batch_size do\n",
    "    inputs, outputs = next_batch(i)\n",
    "    \n",
    "    local feval = function(x)\n",
    "        parameters:copy(x)\n",
    "        grad_parameters:zero()\n",
    "        \n",
    "        local f = 0\n",
    "        for i = 1, #inputs do\n",
    "            local output = model:forward(inputs[i])\n",
    "            local err = criterion:forward(output, targets[i])\n",
    "            f = f + err\n",
    "            local df_do = criterion:backward(output, targets[i])\n",
    "            model:backwards(inputs[i], df_do)\n",
    "        end\n",
    "        \n",
    "        grad_parameters:div(#inputs)\n",
    "        f = f / #inputs\n",
    "        return f, grad_parameters\n",
    "    end\n",
    "    \n",
    "    parameters, grad_parameters = model:getParameters()\n",
    "\n",
    "    optim:adadelta(feval, parameters) --, optim_state)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "* **(Kim, 2014):** Convolutional Neural Networks for Sentence Classification by Yoon Kim\n",
    "* **(Zhang and Wallace, 2015):** A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification by Ye Zhang, Byron Wallace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
