{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "* bag of means (word2vec)\n",
    "* label distribution, c_hat vs c\n",
    "* corpus on all movie descriptions vs only the test set (needed for on the fly classification)\n",
    "* normalization\n",
    "* test distinct pred_labels for specific movie in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\u001e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier # binary relevance\n",
    "# from sklearn.multiclass import LabelPowerSetClassifier # not existing anymore\n",
    "\n",
    "from skmultilearn.meta.br import BinaryRelevance\n",
    "from skmultilearn.meta.lp import LabelPowerset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the metrics we want to use for evaluation\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# actual estimators\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMETRICS = {\\n    \"hamming_loss\": hamming_loss,\\n    \"subset_accuracy\": accuracy_score,\\n    \"precision\": precision_score,\\n    \"macro-f1\": partial(f1_score, average=\"macro\"),\\n    \"samples-f1\": partial(f1_score, average=\"samples\"),\\n    \"weighted-f1\": partial(f1_score, average=\"weighted\"),\\n    \"micro-f1\": partial(f1_score, average=\"micro\"),\\n}\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scoring metrics used for evaluation. namely precision, accuracy, hamming loss (recall)\n",
    "# and f_1-score with several different averages\n",
    "METRICS = ['precision_macro', 'recall_macro', 'f1_macro', 'precision_micro', 'recall_micro', 'f1_micro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_csv = {\n",
    "    'prepped_train_X_Doc2Vec_dbow_d100_n5_mc2_t2':'prepped_train_X_Doc2Vec_dbow_d100_n5_mc2_t2.csv',\n",
    "    'prepped_train_X_Doc2Vec_dm_c_d100_n5_w5_mc2_t2': 'prepped_train_X_Doc2Vec_dm_c_d100_n5_w5_mc2_t2.csv',\n",
    "    'prepped_train_X_Doc2Vec_dm_m_d100_n5_w10_mc2_t2': 'prepped_train_X_Doc2Vec_dm_m_d100_n5_w10_mc2_t2.csv',\n",
    "    'prepped_train_X_Doc2Vec_dbow_dmc': 'prepped_train_X_dbow_dmc.csv',\n",
    "    'prepped_train_X_Doc2Vec_dbow_dmm': 'prepped_train_X_dbow_dmm.csv',\n",
    "    'prepped_train_X_bow_tfidf': 'prepped_train_X_bow_tfidf.csv',\n",
    "    'prepped_train_X_bigrams': 'prepped_train_X_bigrams.csv',\n",
    "    'prepped_train_X_trigrams': 'prepped_train_X_trigrams.csv',\n",
    "    'prepped_train_X_bigrams_tfidf': 'prepped_train_X_bigrams_tfidf.csv',\n",
    "    'prepped_train_X_trigrams_tfidf': 'prepped_train_X_trigrams_tfidf.csv',\n",
    "    'prepped_train_X_bow': 'prepped_train_X_bow.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature vectors per word model\n",
    "train_data = {}\n",
    "\n",
    "for f in data_csv.keys():\n",
    "    d = pd.read_csv('../data/' + data_csv[f])\n",
    "\n",
    "    train_data[f] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/prepped_train_y.csv').as_matrix()\n",
    "\n",
    "# remove the first column containing index numbers\n",
    "y = np.delete(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OVR_ESTIMATORS = {\n",
    "    \"OVR Random Forest\": OneVsRestClassifier(RandomForestClassifier(n_estimators = 100)),\n",
    "    \"OVR LinearSVC\": OneVsRestClassifier(LinearSVC(random_state=1)),\n",
    "    \"OVR Gaussian Naive Bayes\": OneVsRestClassifier(GaussianNB()),\n",
    "    \"OVR Bernoulli Naive Bayes\": OneVsRestClassifier(BernoulliNB())\n",
    "}\n",
    "BR_ESTIMATORS = {\n",
    "    \"Random Forest\": BinaryRelevance(RandomForestClassifier(n_estimators = 100)),\n",
    "    \"LinearSVC\": BinaryRelevance(LinearSVC(random_state=1)),\n",
    "    \"Gaussian Naive Bayes\": BinaryRelevance(GaussianNB()),\n",
    "    \"Bernoulli Naive Bayes\": BinaryRelevance(BernoulliNB())\n",
    "}\n",
    "LP_ESTIMATORS = {\n",
    "    \"LP Random Forest\": LabelPowerset(RandomForestClassifier(n_estimators = 100)),\n",
    "    \"LP LinearSVC\": LabelPowerset(LinearSVC(random_state=1)),\n",
    "    \"LP Gaussian Naive Bayes\": LabelPowerset(GaussianNB()),\n",
    "    \"LP Bernoulli Naive Bayes\": LabelPowerset(BernoulliNB())\n",
    "}\n",
    "\n",
    "# merge all dicts\n",
    "ESTIMATORS = BR_ESTIMATORS.copy()\n",
    "ESTIMATORS.update(LP_ESTIMATORS)\n",
    "# ESTIMATORS.update(OVR_ESTIMATOR)\n",
    "#ESTIMATORS = OVR_ESTIMATORS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.base import clone as sk_clone\n",
    "import time\n",
    "\n",
    "def train(e, X, y):\n",
    "    \"\"\"\n",
    "    Train all the estimators on the current dataset.\n",
    "    The fit method should reset internals anyway.\n",
    "    \"\"\"\n",
    "    e.fit(X, y)\n",
    "\n",
    "def test(e, X, y):\n",
    "    \"\"\" calculating metrics based on the training set \"\"\"\n",
    "    for metric in METRICS:\n",
    "        cv = cross_validation.ShuffleSplit(len(y), random_state=0)\n",
    "        scores = cross_validation.cross_val_score(e, X, y, cv=cv, scoring=metric)\n",
    "\n",
    "        print \"\\t\\tmean %s: %s\" % (metric, scores.sum() / 10)\n",
    "\n",
    "def run_est(X, y):\n",
    "    \"\"\"\n",
    "    Train and test the estimators on the given dataset\n",
    "    \"\"\"\n",
    "    tic = time.time()\n",
    "\n",
    "    # all means of given METRICS\n",
    "    means = []\n",
    "\n",
    "    for e_name, e in ESTIMATORS.items():\n",
    "        print \"\\t-> testing \", e_name\n",
    "\n",
    "        ms = test(e, X, y)\n",
    "        print \"\\t-> %ds elapsed for testing\" % (time.time() - tic,)\n",
    "\n",
    "        means.append(ms)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary relevance training\n",
    "ESTIMATORS = BR_ESTIMATORS.copy()\n",
    "for k in train_data.keys():\n",
    "    # convert pandas dataframe to np.array\n",
    "    X = train_data[k].as_matrix()\n",
    "    # remove continuous index numbers\n",
    "    X = np.delete(X, 0, 1)\n",
    "\n",
    "    assert(X.shape[0] == len(y))\n",
    "\n",
    "    print \"[#] Dataset: \" + k\n",
    "    run_est(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# label powerset training\n",
    "ESTIMATORS = LP_ESTIMATORS\n",
    "\n",
    "for k in train_data.keys():\n",
    "    # convert pandas dataframe to np.array\n",
    "    X = train_data[k].as_matrix()\n",
    "    # remove continuous index numbers\n",
    "    X = np.delete(X, 0, 1)\n",
    "\n",
    "    assert(X.shape[0] == len(y))\n",
    "\n",
    "    print \"[#] Dataset: \" + k\n",
    "    run_est(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('say \"Done.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# let's save the models\n",
    "for e_name, e in ESTIMATORS.items():\n",
    "    with open('../data/estimator_' + e_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(e, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_labels(labels):\n",
    "    atmos = [\"food_for_thought\", \"funny\", \"action\", \"emotional\", \"romantic\", \"dark\", \"brutal\", \"thrilling\"]\n",
    "    \n",
    "    for a, l in zip(atmos, labels):\n",
    "        if l:\n",
    "            print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# n-grams (for bow is n = 1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the normal n-gram models are typically high dimensional sparse vector spaces\n",
    "# with *lots* of zeroes.\n",
    "# i.e. \"bow_tfidf\" will be of shape (2146, 7000),\n",
    "# while \"bow\" will be (2146, 40008). 2146 is the \n",
    "# number of examples, the second number is the size\n",
    "# of the vector representing each training document.\n",
    "named_models = {\n",
    "    'bow': CountVectorizer(min_df=1),\n",
    "    'bigrams': CountVectorizer(ngram_range=(1, 2),stop_words='english'),\n",
    "    'trigrams': CountVectorizer(ngram_range=(1, 3), stop_words='english'),\n",
    "    'bow_tfidf': TfidfVectorizer(stop_words='english'),\n",
    "    'bigrams_tfidf': TfidfVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
    "    'trigrams_tfidf': TfidfVectorizer(ngram_range=(1, 3), stop_words='english'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atmosphere_train_data = pd.read_csv('../data/atmosphere_train.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for idx, row in atmosphere_train_data.iterrows():\n",
    "    if idx > 0:# and idx < 50:\n",
    "        corpus.append(row[\"descr\"])\n",
    "\n",
    "# train the models on the corpus\n",
    "X = []\n",
    "\n",
    "for model in named_models.keys():\n",
    "    # train on the corpus\n",
    "    named_models[model].fit_transform(corpus)\n",
    "    \n",
    "    # extract test data\n",
    "    x = named_models[model].transform(corpus)\n",
    "    X += [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_wordlist(plot):\n",
    "    \"\"\"\n",
    "    Function that cleans the movie description text. Removes\n",
    "    non-alphabetical letters.\n",
    "    \"\"\"\n",
    "    # first step is to remove non-alphabetical characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", plot)\n",
    "    \n",
    "    return text.lower().split()\n",
    "\n",
    "def get_feature_vec(plot, model):\n",
    "    return model.transform(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food_for_thought, funny, action, emotional, romantic, dark, brutal, thrilling\n",
      "#  [0 0 0 0 1 1 1 0]\n",
      "#  [1 1 1 1 0 0 1 1]\n",
      "#  [1 1 1 1 0 0 1 1]\n",
      "#  [1 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# let's test an actual movie! movie: \"her\"\n",
    "plot = \"\"\"Theodore is a lonely man in the final stages of his divorce. When he's not working as a letter writer, his down time is spent playing video games and occasionally hanging out with friends. He decides to purchase the new OS1, which is advertised as the world's first artificially intelligent operating system, \"It's not just an operating system, it's a consciousness,\" the ad states. Theodore quickly finds himself drawn in with Samantha, the voice behind his OS1. As they start spending time together they grow closer and closer and eventually find themselves in love. Having fallen in love with his OS, Theodore finds himself dealing with feelings of both great joy and doubt. As an OS, Samantha has powerful intelligence that she uses to help Theodore in ways others hadn't, but how does she help him deal with his inner conflict of being in love with an OS?\"\"\"\n",
    "X_i = get_feature_vec(make_wordlist(plot), named_models['bow_tfidf']).toarray()\n",
    "\n",
    "\n",
    "print \"food_for_thought, funny, action, emotional, romantic, dark, brutal, thrilling\"\n",
    "for d in data[10][5]:\n",
    "    y_pred = d.predict(X_i)\n",
    "\n",
    "    print \"# \", y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food_for_thought, funny, action, emotional, romantic, dark, brutal, thrilling\n",
      "#  BR Gaussian Naive Bayes [1 1 0 1 1 1 0 0]\n",
      "#  BR LinearSVC [1 1 1 1 0 0 1 1]\n",
      "#  BR Random Forest [1 1 1 1 0 0 1 1]\n",
      "#  BR Bernoulli Naive Bayes [1 1 1 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# movie: forrest gump\n",
    "plot = \"\"\"Forrest Gump is a simple man with a low I.Q. but good intentions. He is running through childhood with his best and only friend Jenny. His 'mama' teaches him the ways of life and leaves him to choose his destiny. Forrest joins the army for service in Vietnam, finding new friends called Dan and Bubba, he wins medals, creates a famous shrimp fishing fleet, inspires people to jog, starts a ping-pong craze, create the smiley, write bumper stickers and songs, donating to people and meeting the president several times. However, this is all irrelevant to Forrest who can only think of his childhood sweetheart Jenny Curran. Who has messed up her life. Although in the end all he wants to prove is that anyone can love anyone.\"\"\"\n",
    "X_i = get_feature_vec(make_wordlist(plot), named_models['bow_tfidf']).toarray()\n",
    "\n",
    "print \"food_for_thought, funny, action, emotional, romantic, dark, brutal, thrilling\"\n",
    "for e_name, e in ESTIMATORS.items():\n",
    "    y_pred = e.predict(X_i)\n",
    "\n",
    "    print \"# \", e_name, y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
